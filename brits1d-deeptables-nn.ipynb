{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4858d56b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-16T14:16:11.516298Z",
     "iopub.status.busy": "2024-11-16T14:16:11.515859Z",
     "iopub.status.idle": "2024-11-16T14:17:41.040099Z",
     "shell.execute_reply": "2024-11-16T14:17:41.038902Z"
    },
    "papermill": {
     "duration": 89.535721,
     "end_time": "2024-11-16T14:17:41.042800",
     "exception": false,
     "start_time": "2024-11-16T14:16:11.507079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorstore 0.1.66 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0 -q\n",
    "!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5 -q\n",
    "!pip install --no-index -U --find-links=/kaggle/input/fix-deeptables/deeptables-0.2.6 deeptables==0.2.6 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b11c135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:17:41.057128Z",
     "iopub.status.busy": "2024-11-16T14:17:41.056470Z",
     "iopub.status.idle": "2024-11-16T14:17:56.512714Z",
     "shell.execute_reply": "2024-11-16T14:17:56.511618Z"
    },
    "papermill": {
     "duration": 15.465584,
     "end_time": "2024-11-16T14:17:56.514829",
     "exception": false,
     "start_time": "2024-11-16T14:17:41.049245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 14:17:43.833263: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-16 14:17:43.833340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-16 14:17:43.835299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0, GPU = True\n",
      "DeepTables version: 0.2.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import tensorflow as tf, deeptables as dt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from deeptables.models import DeepTable, ModelConfig\n",
    "from deeptables.models import deepnets\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('TensorFlow version:',tf.__version__+',',\n",
    "      'GPU =',tf.test.is_gpu_available())\n",
    "print('DeepTables version:',dt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d43d8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:17:56.529048Z",
     "iopub.status.busy": "2024-11-16T14:17:56.528297Z",
     "iopub.status.idle": "2024-11-16T14:17:56.534378Z",
     "shell.execute_reply": "2024-11-16T14:17:56.533485Z"
    },
    "papermill": {
     "duration": 0.015518,
     "end_time": "2024-11-16T14:17:56.536444",
     "exception": false,
     "start_time": "2024-11-16T14:17:56.520926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8ff5e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:17:56.550906Z",
     "iopub.status.busy": "2024-11-16T14:17:56.550606Z",
     "iopub.status.idle": "2024-11-16T14:17:57.210005Z",
     "shell.execute_reply": "2024-11-16T14:17:57.208666Z"
    },
    "papermill": {
     "duration": 0.669362,
     "end_time": "2024-11-16T14:17:57.212978",
     "exception": false,
     "start_time": "2024-11-16T14:17:56.543616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f598fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:17:57.226271Z",
     "iopub.status.busy": "2024-11-16T14:17:57.225915Z",
     "iopub.status.idle": "2024-11-16T14:18:11.146831Z",
     "shell.execute_reply": "2024-11-16T14:18:11.145877Z"
    },
    "papermill": {
     "duration": 13.93217,
     "end_time": "2024-11-16T14:18:11.151188",
     "exception": false,
     "start_time": "2024-11-16T14:17:57.219018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_num</th>\n",
       "      <th>time</th>\n",
       "      <th>bg-5:55</th>\n",
       "      <th>bg-5:50</th>\n",
       "      <th>bg-5:45</th>\n",
       "      <th>bg-5:40</th>\n",
       "      <th>bg-5:35</th>\n",
       "      <th>bg-5:30</th>\n",
       "      <th>bg-5:25</th>\n",
       "      <th>bg-5:20</th>\n",
       "      <th>bg-5:15</th>\n",
       "      <th>bg-5:10</th>\n",
       "      <th>bg-5:05</th>\n",
       "      <th>bg-5:00</th>\n",
       "      <th>bg-4:55</th>\n",
       "      <th>bg-4:50</th>\n",
       "      <th>bg-4:45</th>\n",
       "      <th>bg-4:40</th>\n",
       "      <th>bg-4:35</th>\n",
       "      <th>bg-4:30</th>\n",
       "      <th>bg-4:25</th>\n",
       "      <th>bg-4:20</th>\n",
       "      <th>bg-4:15</th>\n",
       "      <th>bg-4:10</th>\n",
       "      <th>bg-4:05</th>\n",
       "      <th>bg-4:00</th>\n",
       "      <th>bg-3:55</th>\n",
       "      <th>bg-3:50</th>\n",
       "      <th>bg-3:45</th>\n",
       "      <th>bg-3:40</th>\n",
       "      <th>bg-3:35</th>\n",
       "      <th>bg-3:30</th>\n",
       "      <th>bg-3:25</th>\n",
       "      <th>bg-3:20</th>\n",
       "      <th>bg-3:15</th>\n",
       "      <th>bg-3:10</th>\n",
       "      <th>bg-3:05</th>\n",
       "      <th>bg-3:00</th>\n",
       "      <th>bg-2:55</th>\n",
       "      <th>bg-2:50</th>\n",
       "      <th>bg-2:45</th>\n",
       "      <th>bg-2:40</th>\n",
       "      <th>bg-2:35</th>\n",
       "      <th>bg-2:30</th>\n",
       "      <th>bg-2:25</th>\n",
       "      <th>bg-2:20</th>\n",
       "      <th>bg-2:15</th>\n",
       "      <th>bg-2:10</th>\n",
       "      <th>bg-2:05</th>\n",
       "      <th>bg-2:00</th>\n",
       "      <th>bg-1:55</th>\n",
       "      <th>bg-1:50</th>\n",
       "      <th>bg-1:45</th>\n",
       "      <th>bg-1:40</th>\n",
       "      <th>bg-1:35</th>\n",
       "      <th>bg-1:30</th>\n",
       "      <th>bg-1:25</th>\n",
       "      <th>bg-1:20</th>\n",
       "      <th>bg-1:15</th>\n",
       "      <th>bg-1:10</th>\n",
       "      <th>bg-1:05</th>\n",
       "      <th>bg-1:00</th>\n",
       "      <th>bg-0:55</th>\n",
       "      <th>bg-0:50</th>\n",
       "      <th>bg-0:45</th>\n",
       "      <th>bg-0:40</th>\n",
       "      <th>bg-0:35</th>\n",
       "      <th>bg-0:30</th>\n",
       "      <th>bg-0:25</th>\n",
       "      <th>bg-0:20</th>\n",
       "      <th>bg-0:15</th>\n",
       "      <th>bg-0:10</th>\n",
       "      <th>bg-0:05</th>\n",
       "      <th>bg-0:00</th>\n",
       "      <th>insulin-5:55</th>\n",
       "      <th>insulin-5:50</th>\n",
       "      <th>insulin-5:45</th>\n",
       "      <th>insulin-5:40</th>\n",
       "      <th>insulin-5:35</th>\n",
       "      <th>insulin-5:30</th>\n",
       "      <th>insulin-5:25</th>\n",
       "      <th>insulin-5:20</th>\n",
       "      <th>insulin-5:15</th>\n",
       "      <th>insulin-5:10</th>\n",
       "      <th>insulin-5:05</th>\n",
       "      <th>insulin-5:00</th>\n",
       "      <th>insulin-4:55</th>\n",
       "      <th>insulin-4:50</th>\n",
       "      <th>insulin-4:45</th>\n",
       "      <th>insulin-4:40</th>\n",
       "      <th>insulin-4:35</th>\n",
       "      <th>insulin-4:30</th>\n",
       "      <th>insulin-4:25</th>\n",
       "      <th>insulin-4:20</th>\n",
       "      <th>insulin-4:15</th>\n",
       "      <th>insulin-4:10</th>\n",
       "      <th>insulin-4:05</th>\n",
       "      <th>insulin-4:00</th>\n",
       "      <th>insulin-3:55</th>\n",
       "      <th>insulin-3:50</th>\n",
       "      <th>insulin-3:45</th>\n",
       "      <th>insulin-3:40</th>\n",
       "      <th>insulin-3:35</th>\n",
       "      <th>insulin-3:30</th>\n",
       "      <th>insulin-3:25</th>\n",
       "      <th>insulin-3:20</th>\n",
       "      <th>insulin-3:15</th>\n",
       "      <th>insulin-3:10</th>\n",
       "      <th>insulin-3:05</th>\n",
       "      <th>insulin-3:00</th>\n",
       "      <th>insulin-2:55</th>\n",
       "      <th>insulin-2:50</th>\n",
       "      <th>insulin-2:45</th>\n",
       "      <th>insulin-2:40</th>\n",
       "      <th>insulin-2:35</th>\n",
       "      <th>insulin-2:30</th>\n",
       "      <th>insulin-2:25</th>\n",
       "      <th>insulin-2:20</th>\n",
       "      <th>insulin-2:15</th>\n",
       "      <th>insulin-2:10</th>\n",
       "      <th>insulin-2:05</th>\n",
       "      <th>insulin-2:00</th>\n",
       "      <th>insulin-1:55</th>\n",
       "      <th>insulin-1:50</th>\n",
       "      <th>insulin-1:45</th>\n",
       "      <th>insulin-1:40</th>\n",
       "      <th>insulin-1:35</th>\n",
       "      <th>insulin-1:30</th>\n",
       "      <th>insulin-1:25</th>\n",
       "      <th>insulin-1:20</th>\n",
       "      <th>insulin-1:15</th>\n",
       "      <th>insulin-1:10</th>\n",
       "      <th>insulin-1:05</th>\n",
       "      <th>insulin-1:00</th>\n",
       "      <th>insulin-0:55</th>\n",
       "      <th>insulin-0:50</th>\n",
       "      <th>insulin-0:45</th>\n",
       "      <th>insulin-0:40</th>\n",
       "      <th>insulin-0:35</th>\n",
       "      <th>insulin-0:30</th>\n",
       "      <th>insulin-0:25</th>\n",
       "      <th>insulin-0:20</th>\n",
       "      <th>insulin-0:15</th>\n",
       "      <th>insulin-0:10</th>\n",
       "      <th>insulin-0:05</th>\n",
       "      <th>insulin-0:00</th>\n",
       "      <th>carbs-5:55</th>\n",
       "      <th>carbs-5:50</th>\n",
       "      <th>carbs-5:45</th>\n",
       "      <th>carbs-5:40</th>\n",
       "      <th>carbs-5:35</th>\n",
       "      <th>carbs-5:30</th>\n",
       "      <th>carbs-5:25</th>\n",
       "      <th>carbs-5:20</th>\n",
       "      <th>carbs-5:15</th>\n",
       "      <th>carbs-5:10</th>\n",
       "      <th>carbs-5:05</th>\n",
       "      <th>carbs-5:00</th>\n",
       "      <th>carbs-4:55</th>\n",
       "      <th>carbs-4:50</th>\n",
       "      <th>carbs-4:45</th>\n",
       "      <th>carbs-4:40</th>\n",
       "      <th>carbs-4:35</th>\n",
       "      <th>carbs-4:30</th>\n",
       "      <th>carbs-4:25</th>\n",
       "      <th>carbs-4:20</th>\n",
       "      <th>carbs-4:15</th>\n",
       "      <th>carbs-4:10</th>\n",
       "      <th>carbs-4:05</th>\n",
       "      <th>carbs-4:00</th>\n",
       "      <th>carbs-3:55</th>\n",
       "      <th>carbs-3:50</th>\n",
       "      <th>carbs-3:45</th>\n",
       "      <th>carbs-3:40</th>\n",
       "      <th>carbs-3:35</th>\n",
       "      <th>carbs-3:30</th>\n",
       "      <th>carbs-3:25</th>\n",
       "      <th>carbs-3:20</th>\n",
       "      <th>carbs-3:15</th>\n",
       "      <th>carbs-3:10</th>\n",
       "      <th>carbs-3:05</th>\n",
       "      <th>carbs-3:00</th>\n",
       "      <th>carbs-2:55</th>\n",
       "      <th>carbs-2:50</th>\n",
       "      <th>carbs-2:45</th>\n",
       "      <th>carbs-2:40</th>\n",
       "      <th>carbs-2:35</th>\n",
       "      <th>carbs-2:30</th>\n",
       "      <th>carbs-2:25</th>\n",
       "      <th>carbs-2:20</th>\n",
       "      <th>carbs-2:15</th>\n",
       "      <th>carbs-2:10</th>\n",
       "      <th>carbs-2:05</th>\n",
       "      <th>carbs-2:00</th>\n",
       "      <th>carbs-1:55</th>\n",
       "      <th>carbs-1:50</th>\n",
       "      <th>carbs-1:45</th>\n",
       "      <th>carbs-1:40</th>\n",
       "      <th>carbs-1:35</th>\n",
       "      <th>carbs-1:30</th>\n",
       "      <th>carbs-1:25</th>\n",
       "      <th>carbs-1:20</th>\n",
       "      <th>carbs-1:15</th>\n",
       "      <th>carbs-1:10</th>\n",
       "      <th>carbs-1:05</th>\n",
       "      <th>carbs-1:00</th>\n",
       "      <th>carbs-0:55</th>\n",
       "      <th>carbs-0:50</th>\n",
       "      <th>carbs-0:45</th>\n",
       "      <th>carbs-0:40</th>\n",
       "      <th>carbs-0:35</th>\n",
       "      <th>carbs-0:30</th>\n",
       "      <th>carbs-0:25</th>\n",
       "      <th>carbs-0:20</th>\n",
       "      <th>carbs-0:15</th>\n",
       "      <th>carbs-0:10</th>\n",
       "      <th>carbs-0:05</th>\n",
       "      <th>carbs-0:00</th>\n",
       "      <th>hr-5:55</th>\n",
       "      <th>hr-5:50</th>\n",
       "      <th>hr-5:45</th>\n",
       "      <th>hr-5:40</th>\n",
       "      <th>hr-5:35</th>\n",
       "      <th>hr-5:30</th>\n",
       "      <th>hr-5:25</th>\n",
       "      <th>hr-5:20</th>\n",
       "      <th>hr-5:15</th>\n",
       "      <th>hr-5:10</th>\n",
       "      <th>hr-5:05</th>\n",
       "      <th>hr-5:00</th>\n",
       "      <th>hr-4:55</th>\n",
       "      <th>hr-4:50</th>\n",
       "      <th>hr-4:45</th>\n",
       "      <th>hr-4:40</th>\n",
       "      <th>hr-4:35</th>\n",
       "      <th>hr-4:30</th>\n",
       "      <th>hr-4:25</th>\n",
       "      <th>hr-4:20</th>\n",
       "      <th>hr-4:15</th>\n",
       "      <th>hr-4:10</th>\n",
       "      <th>hr-4:05</th>\n",
       "      <th>hr-4:00</th>\n",
       "      <th>hr-3:55</th>\n",
       "      <th>hr-3:50</th>\n",
       "      <th>hr-3:45</th>\n",
       "      <th>hr-3:40</th>\n",
       "      <th>hr-3:35</th>\n",
       "      <th>hr-3:30</th>\n",
       "      <th>hr-3:25</th>\n",
       "      <th>hr-3:20</th>\n",
       "      <th>hr-3:15</th>\n",
       "      <th>hr-3:10</th>\n",
       "      <th>hr-3:05</th>\n",
       "      <th>hr-3:00</th>\n",
       "      <th>hr-2:55</th>\n",
       "      <th>hr-2:50</th>\n",
       "      <th>hr-2:45</th>\n",
       "      <th>hr-2:40</th>\n",
       "      <th>hr-2:35</th>\n",
       "      <th>hr-2:30</th>\n",
       "      <th>hr-2:25</th>\n",
       "      <th>hr-2:20</th>\n",
       "      <th>hr-2:15</th>\n",
       "      <th>hr-2:10</th>\n",
       "      <th>hr-2:05</th>\n",
       "      <th>hr-2:00</th>\n",
       "      <th>hr-1:55</th>\n",
       "      <th>hr-1:50</th>\n",
       "      <th>hr-1:45</th>\n",
       "      <th>hr-1:40</th>\n",
       "      <th>hr-1:35</th>\n",
       "      <th>hr-1:30</th>\n",
       "      <th>hr-1:25</th>\n",
       "      <th>hr-1:20</th>\n",
       "      <th>hr-1:15</th>\n",
       "      <th>hr-1:10</th>\n",
       "      <th>hr-1:05</th>\n",
       "      <th>hr-1:00</th>\n",
       "      <th>hr-0:55</th>\n",
       "      <th>hr-0:50</th>\n",
       "      <th>hr-0:45</th>\n",
       "      <th>hr-0:40</th>\n",
       "      <th>hr-0:35</th>\n",
       "      <th>hr-0:30</th>\n",
       "      <th>hr-0:25</th>\n",
       "      <th>hr-0:20</th>\n",
       "      <th>hr-0:15</th>\n",
       "      <th>hr-0:10</th>\n",
       "      <th>hr-0:05</th>\n",
       "      <th>hr-0:00</th>\n",
       "      <th>steps-5:55</th>\n",
       "      <th>steps-5:50</th>\n",
       "      <th>steps-5:45</th>\n",
       "      <th>steps-5:40</th>\n",
       "      <th>steps-5:35</th>\n",
       "      <th>steps-5:30</th>\n",
       "      <th>steps-5:25</th>\n",
       "      <th>steps-5:20</th>\n",
       "      <th>steps-5:15</th>\n",
       "      <th>steps-5:10</th>\n",
       "      <th>steps-5:05</th>\n",
       "      <th>steps-5:00</th>\n",
       "      <th>steps-4:55</th>\n",
       "      <th>steps-4:50</th>\n",
       "      <th>steps-4:45</th>\n",
       "      <th>steps-4:40</th>\n",
       "      <th>steps-4:35</th>\n",
       "      <th>steps-4:30</th>\n",
       "      <th>steps-4:25</th>\n",
       "      <th>steps-4:20</th>\n",
       "      <th>steps-4:15</th>\n",
       "      <th>steps-4:10</th>\n",
       "      <th>steps-4:05</th>\n",
       "      <th>steps-4:00</th>\n",
       "      <th>steps-3:55</th>\n",
       "      <th>steps-3:50</th>\n",
       "      <th>steps-3:45</th>\n",
       "      <th>steps-3:40</th>\n",
       "      <th>steps-3:35</th>\n",
       "      <th>steps-3:30</th>\n",
       "      <th>steps-3:25</th>\n",
       "      <th>steps-3:20</th>\n",
       "      <th>steps-3:15</th>\n",
       "      <th>steps-3:10</th>\n",
       "      <th>steps-3:05</th>\n",
       "      <th>steps-3:00</th>\n",
       "      <th>steps-2:55</th>\n",
       "      <th>steps-2:50</th>\n",
       "      <th>steps-2:45</th>\n",
       "      <th>steps-2:40</th>\n",
       "      <th>steps-2:35</th>\n",
       "      <th>steps-2:30</th>\n",
       "      <th>steps-2:25</th>\n",
       "      <th>steps-2:20</th>\n",
       "      <th>steps-2:15</th>\n",
       "      <th>steps-2:10</th>\n",
       "      <th>steps-2:05</th>\n",
       "      <th>steps-2:00</th>\n",
       "      <th>steps-1:55</th>\n",
       "      <th>steps-1:50</th>\n",
       "      <th>steps-1:45</th>\n",
       "      <th>steps-1:40</th>\n",
       "      <th>steps-1:35</th>\n",
       "      <th>steps-1:30</th>\n",
       "      <th>steps-1:25</th>\n",
       "      <th>steps-1:20</th>\n",
       "      <th>steps-1:15</th>\n",
       "      <th>steps-1:10</th>\n",
       "      <th>steps-1:05</th>\n",
       "      <th>steps-1:00</th>\n",
       "      <th>steps-0:55</th>\n",
       "      <th>steps-0:50</th>\n",
       "      <th>steps-0:45</th>\n",
       "      <th>steps-0:40</th>\n",
       "      <th>steps-0:35</th>\n",
       "      <th>steps-0:30</th>\n",
       "      <th>steps-0:25</th>\n",
       "      <th>steps-0:20</th>\n",
       "      <th>steps-0:15</th>\n",
       "      <th>steps-0:10</th>\n",
       "      <th>steps-0:05</th>\n",
       "      <th>steps-0:00</th>\n",
       "      <th>cals-5:55</th>\n",
       "      <th>cals-5:50</th>\n",
       "      <th>cals-5:45</th>\n",
       "      <th>cals-5:40</th>\n",
       "      <th>cals-5:35</th>\n",
       "      <th>cals-5:30</th>\n",
       "      <th>cals-5:25</th>\n",
       "      <th>cals-5:20</th>\n",
       "      <th>cals-5:15</th>\n",
       "      <th>cals-5:10</th>\n",
       "      <th>cals-5:05</th>\n",
       "      <th>cals-5:00</th>\n",
       "      <th>cals-4:55</th>\n",
       "      <th>cals-4:50</th>\n",
       "      <th>cals-4:45</th>\n",
       "      <th>cals-4:40</th>\n",
       "      <th>cals-4:35</th>\n",
       "      <th>cals-4:30</th>\n",
       "      <th>cals-4:25</th>\n",
       "      <th>cals-4:20</th>\n",
       "      <th>cals-4:15</th>\n",
       "      <th>cals-4:10</th>\n",
       "      <th>cals-4:05</th>\n",
       "      <th>cals-4:00</th>\n",
       "      <th>cals-3:55</th>\n",
       "      <th>cals-3:50</th>\n",
       "      <th>cals-3:45</th>\n",
       "      <th>cals-3:40</th>\n",
       "      <th>cals-3:35</th>\n",
       "      <th>cals-3:30</th>\n",
       "      <th>cals-3:25</th>\n",
       "      <th>cals-3:20</th>\n",
       "      <th>cals-3:15</th>\n",
       "      <th>cals-3:10</th>\n",
       "      <th>cals-3:05</th>\n",
       "      <th>cals-3:00</th>\n",
       "      <th>cals-2:55</th>\n",
       "      <th>cals-2:50</th>\n",
       "      <th>cals-2:45</th>\n",
       "      <th>cals-2:40</th>\n",
       "      <th>cals-2:35</th>\n",
       "      <th>cals-2:30</th>\n",
       "      <th>cals-2:25</th>\n",
       "      <th>cals-2:20</th>\n",
       "      <th>cals-2:15</th>\n",
       "      <th>cals-2:10</th>\n",
       "      <th>cals-2:05</th>\n",
       "      <th>cals-2:00</th>\n",
       "      <th>cals-1:55</th>\n",
       "      <th>cals-1:50</th>\n",
       "      <th>cals-1:45</th>\n",
       "      <th>cals-1:40</th>\n",
       "      <th>cals-1:35</th>\n",
       "      <th>cals-1:30</th>\n",
       "      <th>cals-1:25</th>\n",
       "      <th>cals-1:20</th>\n",
       "      <th>cals-1:15</th>\n",
       "      <th>cals-1:10</th>\n",
       "      <th>cals-1:05</th>\n",
       "      <th>cals-1:00</th>\n",
       "      <th>cals-0:55</th>\n",
       "      <th>cals-0:50</th>\n",
       "      <th>cals-0:45</th>\n",
       "      <th>cals-0:40</th>\n",
       "      <th>cals-0:35</th>\n",
       "      <th>cals-0:30</th>\n",
       "      <th>cals-0:25</th>\n",
       "      <th>cals-0:20</th>\n",
       "      <th>cals-0:15</th>\n",
       "      <th>cals-0:10</th>\n",
       "      <th>cals-0:05</th>\n",
       "      <th>cals-0:00</th>\n",
       "      <th>activity-5:55</th>\n",
       "      <th>activity-5:50</th>\n",
       "      <th>activity-5:45</th>\n",
       "      <th>activity-5:40</th>\n",
       "      <th>activity-5:35</th>\n",
       "      <th>activity-5:30</th>\n",
       "      <th>activity-5:25</th>\n",
       "      <th>activity-5:20</th>\n",
       "      <th>activity-5:15</th>\n",
       "      <th>activity-5:10</th>\n",
       "      <th>activity-5:05</th>\n",
       "      <th>activity-5:00</th>\n",
       "      <th>activity-4:55</th>\n",
       "      <th>activity-4:50</th>\n",
       "      <th>activity-4:45</th>\n",
       "      <th>activity-4:40</th>\n",
       "      <th>activity-4:35</th>\n",
       "      <th>activity-4:30</th>\n",
       "      <th>activity-4:25</th>\n",
       "      <th>activity-4:20</th>\n",
       "      <th>activity-4:15</th>\n",
       "      <th>activity-4:10</th>\n",
       "      <th>activity-4:05</th>\n",
       "      <th>activity-4:00</th>\n",
       "      <th>activity-3:55</th>\n",
       "      <th>activity-3:50</th>\n",
       "      <th>activity-3:45</th>\n",
       "      <th>activity-3:40</th>\n",
       "      <th>activity-3:35</th>\n",
       "      <th>activity-3:30</th>\n",
       "      <th>activity-3:25</th>\n",
       "      <th>activity-3:20</th>\n",
       "      <th>activity-3:15</th>\n",
       "      <th>activity-3:10</th>\n",
       "      <th>activity-3:05</th>\n",
       "      <th>activity-3:00</th>\n",
       "      <th>activity-2:55</th>\n",
       "      <th>activity-2:50</th>\n",
       "      <th>activity-2:45</th>\n",
       "      <th>activity-2:40</th>\n",
       "      <th>activity-2:35</th>\n",
       "      <th>activity-2:30</th>\n",
       "      <th>activity-2:25</th>\n",
       "      <th>activity-2:20</th>\n",
       "      <th>activity-2:15</th>\n",
       "      <th>activity-2:10</th>\n",
       "      <th>activity-2:05</th>\n",
       "      <th>activity-2:00</th>\n",
       "      <th>activity-1:55</th>\n",
       "      <th>activity-1:50</th>\n",
       "      <th>activity-1:45</th>\n",
       "      <th>activity-1:40</th>\n",
       "      <th>activity-1:35</th>\n",
       "      <th>activity-1:30</th>\n",
       "      <th>activity-1:25</th>\n",
       "      <th>activity-1:20</th>\n",
       "      <th>activity-1:15</th>\n",
       "      <th>activity-1:10</th>\n",
       "      <th>activity-1:05</th>\n",
       "      <th>activity-1:00</th>\n",
       "      <th>activity-0:55</th>\n",
       "      <th>activity-0:50</th>\n",
       "      <th>activity-0:45</th>\n",
       "      <th>activity-0:40</th>\n",
       "      <th>activity-0:35</th>\n",
       "      <th>activity-0:30</th>\n",
       "      <th>activity-0:25</th>\n",
       "      <th>activity-0:20</th>\n",
       "      <th>activity-0:15</th>\n",
       "      <th>activity-0:10</th>\n",
       "      <th>activity-0:05</th>\n",
       "      <th>activity-0:00</th>\n",
       "      <th>bg+1:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p01_0</th>\n",
       "      <td>p01</td>\n",
       "      <td>2024-11-16 06:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p01_1</th>\n",
       "      <td>p01</td>\n",
       "      <td>2024-11-16 06:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p01_2</th>\n",
       "      <td>p01</td>\n",
       "      <td>2024-11-16 06:40:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p01_3</th>\n",
       "      <td>p01</td>\n",
       "      <td>2024-11-16 06:55:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p01_4</th>\n",
       "      <td>p01</td>\n",
       "      <td>2024-11-16 07:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p_num                time  bg-5:55  bg-5:50  bg-5:45  bg-5:40  bg-5:35  bg-5:30  bg-5:25  bg-5:20  bg-5:15  bg-5:10  bg-5:05  bg-5:00  bg-4:55  bg-4:50  bg-4:45  bg-4:40  bg-4:35  bg-4:30  bg-4:25  bg-4:20  bg-4:15  bg-4:10  bg-4:05  bg-4:00  bg-3:55  bg-3:50  bg-3:45  bg-3:40  bg-3:35  bg-3:30  bg-3:25  bg-3:20  bg-3:15  bg-3:10  bg-3:05  bg-3:00  bg-2:55  bg-2:50  bg-2:45  bg-2:40  bg-2:35  bg-2:30  bg-2:25  bg-2:20  bg-2:15  bg-2:10  bg-2:05  bg-2:00  bg-1:55  bg-1:50  bg-1:45  bg-1:40  bg-1:35  bg-1:30  bg-1:25  bg-1:20  bg-1:15  bg-1:10  bg-1:05  bg-1:00  bg-0:55  bg-0:50  bg-0:45  bg-0:40  bg-0:35  bg-0:30  bg-0:25  bg-0:20  bg-0:15  bg-0:10  bg-0:05  bg-0:00  insulin-5:55  insulin-5:50  insulin-5:45  insulin-5:40  insulin-5:35  insulin-5:30  insulin-5:25  insulin-5:20  insulin-5:15  insulin-5:10  insulin-5:05  insulin-5:00  insulin-4:55  insulin-4:50  insulin-4:45  insulin-4:40  insulin-4:35  insulin-4:30  insulin-4:25  insulin-4:20  insulin-4:15  insulin-4:10  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "p01_0   p01 2024-11-16 06:10:00      NaN      NaN      9.6      NaN      NaN      9.7      NaN      NaN      9.2      NaN      NaN      8.7      NaN      NaN      8.4      NaN      NaN      8.1      NaN      NaN      8.3      NaN      NaN      9.6      NaN      NaN     11.1      NaN      NaN     11.8      NaN      NaN     12.8      NaN      NaN     13.9      NaN      NaN     14.2      NaN      NaN     14.2      NaN      NaN     15.4      NaN      NaN     17.2      NaN      NaN     18.2      NaN      NaN     18.4      NaN      NaN     18.0      NaN      NaN     17.3      NaN      NaN     17.5      NaN      NaN     17.3      NaN      NaN     16.2      NaN      NaN     15.1        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083   \n",
       "p01_1   p01 2024-11-16 06:25:00      NaN      NaN      9.7      NaN      NaN      9.2      NaN      NaN      8.7      NaN      NaN      8.4      NaN      NaN      8.1      NaN      NaN      8.3      NaN      NaN      9.6      NaN      NaN     11.1      NaN      NaN     11.8      NaN      NaN     12.8      NaN      NaN     13.9      NaN      NaN     14.2      NaN      NaN     14.2      NaN      NaN     15.4      NaN      NaN     17.2      NaN      NaN     18.2      NaN      NaN     18.4      NaN      NaN     18.0      NaN      NaN     17.3      NaN      NaN     17.5      NaN      NaN     17.3      NaN      NaN     16.2      NaN      NaN     15.1      NaN      NaN     14.4        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083   \n",
       "p01_2   p01 2024-11-16 06:40:00      NaN      NaN      9.2      NaN      NaN      8.7      NaN      NaN      8.4      NaN      NaN      8.1      NaN      NaN      8.3      NaN      NaN      9.6      NaN      NaN     11.1      NaN      NaN     11.8      NaN      NaN     12.8      NaN      NaN     13.9      NaN      NaN     14.2      NaN      NaN     14.2      NaN      NaN     15.4      NaN      NaN     17.2      NaN      NaN     18.2      NaN      NaN     18.4      NaN      NaN     18.0      NaN      NaN     17.3      NaN      NaN     17.5      NaN      NaN     17.3      NaN      NaN     16.2      NaN      NaN     15.1      NaN      NaN     14.4      NaN      NaN     13.9        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083   \n",
       "p01_3   p01 2024-11-16 06:55:00      NaN      NaN      8.7      NaN      NaN      8.4      NaN      NaN      8.1      NaN      NaN      8.3      NaN      NaN      9.6      NaN      NaN     11.1      NaN      NaN     11.8      NaN      NaN     12.8      NaN      NaN     13.9      NaN      NaN     14.2      NaN      NaN     14.2      NaN      NaN     15.4      NaN      NaN     17.2      NaN      NaN     18.2      NaN      NaN     18.4      NaN      NaN     18.0      NaN      NaN     17.3      NaN      NaN     17.5      NaN      NaN     17.3      NaN      NaN     16.2      NaN      NaN     15.1      NaN      NaN     14.4      NaN      NaN     13.9      NaN      NaN     13.8        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083   \n",
       "p01_4   p01 2024-11-16 07:10:00      NaN      NaN      8.4      NaN      NaN      8.1      NaN      NaN      8.3      NaN      NaN      9.6      NaN      NaN     11.1      NaN      NaN     11.8      NaN      NaN     12.8      NaN      NaN     13.9      NaN      NaN     14.2      NaN      NaN     14.2      NaN      NaN     15.4      NaN      NaN     17.2      NaN      NaN     18.2      NaN      NaN     18.4      NaN      NaN     18.0      NaN      NaN     17.3      NaN      NaN     17.5      NaN      NaN     17.3      NaN      NaN     16.2      NaN      NaN     15.1      NaN      NaN     14.4      NaN      NaN     13.9      NaN      NaN     13.8      NaN      NaN     13.4        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083   \n",
       "\n",
       "       insulin-4:05  insulin-4:00  insulin-3:55  insulin-3:50  insulin-3:45  insulin-3:40  insulin-3:35  insulin-3:30  insulin-3:25  insulin-3:20  insulin-3:15  insulin-3:10  insulin-3:05  insulin-3:00  insulin-2:55  insulin-2:50  insulin-2:45  insulin-2:40  insulin-2:35  insulin-2:30  insulin-2:25  insulin-2:20  insulin-2:15  insulin-2:10  insulin-2:05  insulin-2:00  insulin-1:55  insulin-1:50  insulin-1:45  insulin-1:40  insulin-1:35  insulin-1:30  insulin-1:25  insulin-1:20  insulin-1:15  insulin-1:10  insulin-1:05  insulin-1:00  insulin-0:55  insulin-0:50  insulin-0:45  insulin-0:40  insulin-0:35  insulin-0:30  insulin-0:25  insulin-0:20  insulin-0:15  insulin-0:10  insulin-0:05  insulin-0:00  carbs-5:55  carbs-5:50  carbs-5:45  carbs-5:40  carbs-5:35  carbs-5:30  carbs-5:25  carbs-5:20  carbs-5:15  carbs-5:10  carbs-5:05  carbs-5:00  carbs-4:55  carbs-4:50  carbs-4:45  carbs-4:40  carbs-4:35  carbs-4:30  carbs-4:25  carbs-4:20  carbs-4:15  carbs-4:10  carbs-4:05  carbs-4:00  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "p01_0        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0417        0.0417         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_1        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0417        0.0417        0.0417        0.0417        0.0417         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_2        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_3        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_4        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0083        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0583        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417        0.0417         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "       carbs-3:55  carbs-3:50  carbs-3:45  carbs-3:40  carbs-3:35  carbs-3:30  carbs-3:25  carbs-3:20  carbs-3:15  carbs-3:10  carbs-3:05  carbs-3:00  carbs-2:55  carbs-2:50  carbs-2:45  carbs-2:40  carbs-2:35  carbs-2:30  carbs-2:25  carbs-2:20  carbs-2:15  carbs-2:10  carbs-2:05  carbs-2:00  carbs-1:55  carbs-1:50  carbs-1:45  carbs-1:40  carbs-1:35  carbs-1:30  carbs-1:25  carbs-1:20  carbs-1:15  carbs-1:10  carbs-1:05  carbs-1:00  carbs-0:55  carbs-0:50  carbs-0:45  carbs-0:40  carbs-0:35  carbs-0:30  carbs-0:25  carbs-0:20  carbs-0:15  carbs-0:10  carbs-0:05  carbs-0:00  hr-5:55  hr-5:50  hr-5:45  hr-5:40  hr-5:35  hr-5:30  hr-5:25  hr-5:20  hr-5:15  hr-5:10  hr-5:05  hr-5:00  hr-4:55  hr-4:50  hr-4:45  hr-4:40  hr-4:35  hr-4:30  hr-4:25  hr-4:20  hr-4:15  hr-4:10  hr-4:05  hr-4:00  hr-3:55  hr-3:50  hr-3:45  hr-3:40  hr-3:35  hr-3:30  hr-3:25  hr-3:20  hr-3:15  hr-3:10  hr-3:05  hr-3:00  hr-2:55  hr-2:50  hr-2:45  hr-2:40  hr-2:35  hr-2:30  hr-2:25  hr-2:20  hr-2:15  hr-2:10  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "p01_0         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "p01_1         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "p01_2         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "p01_3         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "p01_4         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "       hr-2:05  hr-2:00  hr-1:55  hr-1:50  hr-1:45  hr-1:40  hr-1:35  hr-1:30  hr-1:25  hr-1:20  hr-1:15  hr-1:10  hr-1:05  hr-1:00  hr-0:55  hr-0:50  hr-0:45  hr-0:40  hr-0:35  hr-0:30  hr-0:25  hr-0:20  hr-0:15  hr-0:10  hr-0:05  hr-0:00  steps-5:55  steps-5:50  steps-5:45  steps-5:40  steps-5:35  steps-5:30  steps-5:25  steps-5:20  steps-5:15  steps-5:10  steps-5:05  steps-5:00  steps-4:55  steps-4:50  steps-4:45  steps-4:40  steps-4:35  steps-4:30  steps-4:25  steps-4:20  steps-4:15  steps-4:10  steps-4:05  steps-4:00  steps-3:55  steps-3:50  steps-3:45  steps-3:40  steps-3:35  steps-3:30  steps-3:25  steps-3:20  steps-3:15  steps-3:10  steps-3:05  steps-3:00  steps-2:55  steps-2:50  steps-2:45  steps-2:40  steps-2:35  steps-2:30  steps-2:25  steps-2:20  steps-2:15  steps-2:10  steps-2:05  steps-2:00  steps-1:55  steps-1:50  steps-1:45  steps-1:40  steps-1:35  steps-1:30  steps-1:25  steps-1:20  steps-1:15  steps-1:10  steps-1:05  steps-1:00  steps-0:55  steps-0:50  steps-0:45  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "p01_0      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_1      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_2      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_3      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "p01_4      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "       steps-0:40  steps-0:35  steps-0:30  steps-0:25  steps-0:20  steps-0:15  steps-0:10  steps-0:05  steps-0:00  cals-5:55  cals-5:50  cals-5:45  cals-5:40  cals-5:35  cals-5:30  cals-5:25  cals-5:20  cals-5:15  cals-5:10  cals-5:05  cals-5:00  cals-4:55  cals-4:50  cals-4:45  cals-4:40  cals-4:35  cals-4:30  cals-4:25  cals-4:20  cals-4:15  cals-4:10  cals-4:05  cals-4:00  cals-3:55  cals-3:50  cals-3:45  cals-3:40  cals-3:35  cals-3:30  cals-3:25  cals-3:20  cals-3:15  cals-3:10  cals-3:05  cals-3:00  cals-2:55  cals-2:50  cals-2:45  cals-2:40  cals-2:35  cals-2:30  cals-2:25  cals-2:20  cals-2:15  cals-2:10  cals-2:05  cals-2:00  cals-1:55  cals-1:50  cals-1:45  cals-1:40  cals-1:35  cals-1:30  cals-1:25  cals-1:20  cals-1:15  cals-1:10  cals-1:05  cals-1:00  cals-0:55  cals-0:50  cals-0:45  cals-0:40  cals-0:35  cals-0:30  cals-0:25  cals-0:20  cals-0:15  cals-0:10  cals-0:05  cals-0:00 activity-5:55 activity-5:50 activity-5:45 activity-5:40 activity-5:35 activity-5:30  \\\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "p01_0         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN           NaN           NaN           NaN           NaN           NaN           NaN   \n",
       "p01_1         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN           NaN           NaN           NaN           NaN           NaN           NaN   \n",
       "p01_2         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN           NaN           NaN           NaN           NaN           NaN           NaN   \n",
       "p01_3         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN           NaN           NaN           NaN           NaN           NaN           NaN   \n",
       "p01_4         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN           NaN           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "      activity-5:25 activity-5:20 activity-5:15 activity-5:10 activity-5:05 activity-5:00 activity-4:55 activity-4:50 activity-4:45 activity-4:40 activity-4:35 activity-4:30 activity-4:25 activity-4:20 activity-4:15 activity-4:10 activity-4:05 activity-4:00 activity-3:55 activity-3:50 activity-3:45 activity-3:40 activity-3:35 activity-3:30 activity-3:25 activity-3:20 activity-3:15 activity-3:10 activity-3:05 activity-3:00 activity-2:55 activity-2:50 activity-2:45 activity-2:40 activity-2:35 activity-2:30 activity-2:25 activity-2:20 activity-2:15 activity-2:10 activity-2:05 activity-2:00 activity-1:55 activity-1:50 activity-1:45 activity-1:40 activity-1:35 activity-1:30 activity-1:25 activity-1:20 activity-1:15 activity-1:10 activity-1:05 activity-1:00 activity-0:55 activity-0:50 activity-0:45 activity-0:40 activity-0:35 activity-0:30 activity-0:25 activity-0:20 activity-0:15 activity-0:10 activity-0:05 activity-0:00  bg+1:00  \n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "p01_0           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN     13.4  \n",
       "p01_1           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN     12.8  \n",
       "p01_2           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN     15.5  \n",
       "p01_3           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN     14.8  \n",
       "p01_4           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN           NaN     12.7  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('/kaggle/input/brist1d/train.csv', index_col='id', parse_dates=['time'])\n",
    "df_test = pd.read_csv('/kaggle/input/brist1d/test.csv', index_col='id', parse_dates=['time'])\n",
    "df_subm = pd.read_csv('/kaggle/input/brist1d/sample_submission.csv')\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db92b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:11.172560Z",
     "iopub.status.busy": "2024-11-16T14:18:11.172234Z",
     "iopub.status.idle": "2024-11-16T14:18:11.178531Z",
     "shell.execute_reply": "2024-11-16T14:18:11.177616Z"
    },
    "papermill": {
     "duration": 0.018786,
     "end_time": "2024-11-16T14:18:11.180622",
     "exception": false,
     "start_time": "2024-11-16T14:18:11.161836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some frameworks may not handle column names with special characters like colons properly\n",
    "df_train.columns = df_train.columns.str.replace(':', '-')\n",
    "df_test.columns = df_test.columns.str.replace(':', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f90f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:11.200641Z",
     "iopub.status.busy": "2024-11-16T14:18:11.200347Z",
     "iopub.status.idle": "2024-11-16T14:18:11.206516Z",
     "shell.execute_reply": "2024-11-16T14:18:11.205672Z"
    },
    "papermill": {
     "duration": 0.018172,
     "end_time": "2024-11-16T14:18:11.208399",
     "exception": false,
     "start_time": "2024-11-16T14:18:11.190227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    hours = range(0, 6, 1)\n",
    "    minutes = range(0, 60, 5)\n",
    "\n",
    "    target_col = 'bg+1-00' # Target column name for prediction\n",
    "    group_col = 'p_num'    # Column name for grouping (e.g., participant number)\n",
    "    date_col = 'time'      # Column name for time data\n",
    "\n",
    "    n_splits = 5\n",
    "    seed_list = [42]\n",
    "    early_stop = 200\n",
    "\n",
    "    TRAIN_CATBOOST = False\n",
    "    TRAIN_XGB = True\n",
    "    TRAIN_LGB = False\n",
    "\n",
    "    xgboost_params = {\n",
    "        'random_state': 42,  # This will be updated per seed\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'learning_rate': 0.025,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cpu',\n",
    "        'max_depth': 7,\n",
    "        'min_child_weight': 200,\n",
    "        'lambda': 200,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d28eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:11.228361Z",
     "iopub.status.busy": "2024-11-16T14:18:11.228063Z",
     "iopub.status.idle": "2024-11-16T14:18:11.235597Z",
     "shell.execute_reply": "2024-11-16T14:18:11.234770Z"
    },
    "papermill": {
     "duration": 0.019519,
     "end_time": "2024-11-16T14:18:11.237414",
     "exception": false,
     "start_time": "2024-11-16T14:18:11.217895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# We only need the last 12 time intervals (1 hour)\n",
    "bg_cols   = [f'bg-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "insu_cols = [f'insulin-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "carb_cols = [f'carbs-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "hr_cols   = [f'hr-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "step_cols = [f'steps-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "cals_cols = [f'cals-{i}-{j:02d}' for i, j in product(CONFIG.hours, CONFIG.minutes)][:12]\n",
    "\n",
    "feature_cols = bg_cols + insu_cols + carb_cols + hr_cols + step_cols + cals_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8834ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:11.257068Z",
     "iopub.status.busy": "2024-11-16T14:18:11.256791Z",
     "iopub.status.idle": "2024-11-16T14:18:34.466440Z",
     "shell.execute_reply": "2024-11-16T14:18:34.465309Z"
    },
    "papermill": {
     "duration": 23.222307,
     "end_time": "2024-11-16T14:18:34.468977",
     "exception": false,
     "start_time": "2024-11-16T14:18:11.246670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for colset in [bg_cols, insu_cols, carb_cols, hr_cols, step_cols, cals_cols]:\n",
    "    df_train[colset] = df_train[colset].interpolate(axis=1)\n",
    "    df_test[colset] = df_test[colset].interpolate(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf44c941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:34.491524Z",
     "iopub.status.busy": "2024-11-16T14:18:34.491038Z",
     "iopub.status.idle": "2024-11-16T14:18:34.938938Z",
     "shell.execute_reply": "2024-11-16T14:18:34.937888Z"
    },
    "papermill": {
     "duration": 0.462296,
     "end_time": "2024-11-16T14:18:34.941421",
     "exception": false,
     "start_time": "2024-11-16T14:18:34.479125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer()\n",
    "\n",
    "df_train[feature_cols] = imputer.fit_transform(df_train[feature_cols])\n",
    "df_test[feature_cols] = imputer.transform(df_test[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abfffade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:34.962687Z",
     "iopub.status.busy": "2024-11-16T14:18:34.962346Z",
     "iopub.status.idle": "2024-11-16T14:18:34.987435Z",
     "shell.execute_reply": "2024-11-16T14:18:34.986722Z"
    },
    "papermill": {
     "duration": 0.037794,
     "end_time": "2024-11-16T14:18:34.989373",
     "exception": false,
     "start_time": "2024-11-16T14:18:34.951579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['sin_hour'] = np.sin(np.pi * df_train[CONFIG.date_col].dt.hour / 12)\n",
    "df_train['cos_hour'] = np.cos(np.pi * df_train[CONFIG.date_col].dt.hour / 12)\n",
    "\n",
    "df_test['sin_hour'] = np.sin(np.pi * df_test[CONFIG.date_col].dt.hour / 12)\n",
    "df_test['cos_hour'] = np.cos(np.pi * df_test[CONFIG.date_col].dt.hour / 12)\n",
    "\n",
    "feature_cols.extend(['sin_hour', 'cos_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e853b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:35.010002Z",
     "iopub.status.busy": "2024-11-16T14:18:35.009716Z",
     "iopub.status.idle": "2024-11-16T14:18:35.301317Z",
     "shell.execute_reply": "2024-11-16T14:18:35.300423Z"
    },
    "papermill": {
     "duration": 0.304221,
     "end_time": "2024-11-16T14:18:35.303348",
     "exception": false,
     "start_time": "2024-11-16T14:18:34.999127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.001 to 0.01 to 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\n",
    "LR_START = 1e-3\n",
    "LR_MAX = 1e-2\n",
    "LR_MIN = 1e-2\n",
    "LR_RAMPUP_EPOCHS = 1\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "EPOCHS = 7\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "        cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "lr_y = [lrfn(x) for x in rng]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rng, lr_y, '-o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('LR')\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n",
    "      format(lr_y[0], max(lr_y), lr_y[-1]))\n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57bfac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:35.325075Z",
     "iopub.status.busy": "2024-11-16T14:18:35.324536Z",
     "iopub.status.idle": "2024-11-16T14:18:35.332720Z",
     "shell.execute_reply": "2024-11-16T14:18:35.331922Z"
    },
    "papermill": {
     "duration": 0.021056,
     "end_time": "2024-11-16T14:18:35.334607",
     "exception": false,
     "start_time": "2024-11-16T14:18:35.313551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    epochs = 1\n",
    "    batch_size = 128\n",
    "    LR_Scheduler = [LR_Scheduler]\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    \n",
    "    conf = ModelConfig(auto_imputation=False,\n",
    "                       auto_discrete=False,\n",
    "                       auto_discard_unique=True,\n",
    "                       categorical_columns='auto',\n",
    "                       apply_gbm_features=True,\n",
    "                       fixed_embedding_dim=True,\n",
    "                       embeddings_output_dim=4,\n",
    "                       embedding_dropout=0.2,\n",
    "                       nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n",
    "                       dnn_params={\n",
    "                           'hidden_units': ((1024, 0.0, True),\n",
    "                                            (512, 0.0, True),\n",
    "                                            (256, 0.0, True),\n",
    "                                            (128, 0.0, True)),\n",
    "                           'dnn_activation': 'relu',\n",
    "                       },\n",
    "                       stacking_op='concat',\n",
    "                       output_use_bias=False,\n",
    "                       optimizer=optimizer,\n",
    "                       task='regression',\n",
    "                       loss='auto',\n",
    "                       metrics=[\"RootMeanSquaredError\"],\n",
    "                       earlystopping_patience=1,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0adb418b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:35.356282Z",
     "iopub.status.busy": "2024-11-16T14:18:35.355694Z",
     "iopub.status.idle": "2024-11-16T14:18:35.368709Z",
     "shell.execute_reply": "2024-11-16T14:18:35.367892Z"
    },
    "papermill": {
     "duration": 0.026349,
     "end_time": "2024-11-16T14:18:35.370885",
     "exception": false,
     "start_time": "2024-11-16T14:18:35.344536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nn(data):\n",
    "\n",
    "    X = data[feature_cols]\n",
    "    y = data[CONFIG.target_col]\n",
    "    groups = data[CONFIG.group_col]\n",
    "\n",
    "    cv = GroupKFold(n_splits=CONFIG.n_splits)\n",
    "    oof = np.zeros(len(data))\n",
    "    models = []\n",
    "    \n",
    "    for fi, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Fold {fi+1}/{CONFIG.n_splits} ...\")\n",
    "        print(\"#\"*25)   \n",
    "        K.clear_session()\n",
    "        model = DeepTable(config=CFG.conf)\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx],\n",
    "                  validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n",
    "                  callbacks=CFG.LR_Scheduler,\n",
    "                  batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n",
    "        models.append(model)\n",
    "        \n",
    "        # Avoid some errors\n",
    "        with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "            for j, var in enumerate(CFG.optimizer.weights):\n",
    "                name = 'variable{}'.format(j)\n",
    "                CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n",
    "        CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n",
    "        \n",
    "        oof_preds = model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n",
    "        rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n",
    "        print(f'\\nFold {fi+1} | rmse: {rmse}\\n')\n",
    "        if fi<CONFIG.n_splits: oof[valid_idx] = oof_preds\n",
    "        else: oof[valid_idx] += oof_preds\n",
    "            \n",
    "    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n",
    "    print(f'Overall CV rmse: {rmse}\\n')\n",
    "    plot_model(model.get_model().model)\n",
    "    return models\n",
    "\n",
    "def infer_nn(data, models):\n",
    "    return np.mean([model.predict(data, verbose=1, batch_size=512).flatten() for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a215daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:18:35.391878Z",
     "iopub.status.busy": "2024-11-16T14:18:35.391577Z",
     "iopub.status.idle": "2024-11-16T14:22:57.987942Z",
     "shell.execute_reply": "2024-11-16T14:22:57.986869Z"
    },
    "papermill": {
     "duration": 262.609606,
     "end_time": "2024-11-16T14:22:57.990368",
     "exception": false,
     "start_time": "2024-11-16T14:18:35.380762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1/5 ...\n",
      "#########################\n",
      "11-16 14:18:35 I deeptables.m.deeptable.py 338 - X.Shape=(150996, 74), y.Shape=(150996,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'cin_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7832c039a8f0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-16 14:18:35 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-16 14:18:35 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:18:35 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-16 14:18:36 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.13408994674682617s\n",
      "11-16 14:18:36 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-16 14:18:36 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.0007784366607666016s\n",
      "11-16 14:18:36 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-16 14:18:36 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.110209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16716\n",
      "[LightGBM] [Info] Number of data points in the train set: 150996, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score 8.224292\n",
      "11-16 14:18:42 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 6.642626762390137s\n",
      "11-16 14:18:43 I deeptables.m.preprocessor.py 198 - fit_transform taken 7.406708717346191s\n",
      "11-16 14:18:43 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:18:43 I deeptables.m.preprocessor.py 251 - transform_X taken 0.13724660873413086s\n",
      "11-16 14:18:43 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-16 14:18:43 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0005426406860351562s\n",
      "11-16 14:18:43 I deeptables.m.deeptable.py 354 - Training...\n",
      "11-16 14:18:43 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:1, mode:min\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "11-16 14:18:43 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:18:46 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:18:46 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-16 14:18:48 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (100)', 'input_continuous_all: (74)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 474)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'cin_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 474), output_shape (None, 128)\n",
      "cin: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-16 14:18:48 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "1179/1179 - 42s - loss: 4.9213 - root_mean_squared_error: 2.2184 - val_loss: 5.4637 - val_root_mean_squared_error: 2.3375 - lr: 0.0010 - 42s/epoch - 36ms/step\n",
      "11-16 14:19:30 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-16 14:19:30 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-16 14:19:31 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:19:31 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:19:31 I deeptables.m.preprocessor.py 251 - transform_X taken 0.14145994186401367s\n",
      "11-16 14:19:31 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:19:31 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "51/51 [==============================] - 2s 24ms/step\n",
      "11-16 14:19:33 I deeptables.m.deeptable.py 559 - predict_proba taken 2.4712576866149902s\n",
      "\n",
      "Fold 1 | rmse: 2.3369\n",
      "\n",
      "#########################\n",
      "### Fold 2/5 ...\n",
      "#########################\n",
      "11-16 14:19:33 I deeptables.m.deeptable.py 338 - X.Shape=(142864, 74), y.Shape=(142864,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'cin_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7832c039a8f0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-16 14:19:33 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-16 14:19:34 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:19:34 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-16 14:19:34 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.1260826587677002s\n",
      "11-16 14:19:34 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-16 14:19:34 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.00077056884765625s\n",
      "11-16 14:19:34 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-16 14:19:34 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16747\n",
      "[LightGBM] [Info] Number of data points in the train set: 142864, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score 8.093053\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 6.099892616271973s\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 198 - fit_transform taken 6.575271844863892s\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 251 - transform_X taken 0.18596649169921875s\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-16 14:19:40 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0007512569427490234s\n",
      "11-16 14:19:40 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "11-16 14:19:40 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:19:42 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:19:42 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-16 14:19:43 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (100)', 'input_continuous_all: (74)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 474)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'cin_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 474), output_shape (None, 128)\n",
      "cin: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-16 14:19:43 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "1116/1116 - 37s - loss: 3.8937 - root_mean_squared_error: 1.9733 - val_loss: 5.4560 - val_root_mean_squared_error: 2.3358 - lr: 0.0010 - 37s/epoch - 33ms/step\n",
      "11-16 14:20:20 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-16 14:20:20 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-16 14:20:21 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:20:21 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:20:21 I deeptables.m.preprocessor.py 251 - transform_X taken 0.18542838096618652s\n",
      "11-16 14:20:21 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:20:21 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "67/67 [==============================] - 2s 21ms/step\n",
      "11-16 14:20:24 I deeptables.m.deeptable.py 559 - predict_proba taken 2.7520639896392822s\n",
      "\n",
      "Fold 2 | rmse: 2.3364\n",
      "\n",
      "#########################\n",
      "### Fold 3/5 ...\n",
      "#########################\n",
      "11-16 14:20:24 I deeptables.m.deeptable.py 338 - X.Shape=(143187, 74), y.Shape=(143187,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'cin_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7832c039a8f0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-16 14:20:24 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-16 14:20:25 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:20:25 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-16 14:20:25 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.12494182586669922s\n",
      "11-16 14:20:25 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-16 14:20:25 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.0007076263427734375s\n",
      "11-16 14:20:25 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-16 14:20:25 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16611\n",
      "[LightGBM] [Info] Number of data points in the train set: 143187, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score 8.576904\n",
      "11-16 14:20:31 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 6.302797317504883s\n",
      "11-16 14:20:31 I deeptables.m.preprocessor.py 198 - fit_transform taken 6.778013467788696s\n",
      "11-16 14:20:31 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:20:32 I deeptables.m.preprocessor.py 251 - transform_X taken 0.1856222152709961s\n",
      "11-16 14:20:32 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-16 14:20:32 I deeptables.m.preprocessor.py 238 - transform_y taken 0.000240325927734375s\n",
      "11-16 14:20:32 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "11-16 14:20:32 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:20:33 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:20:33 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-16 14:20:34 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (100)', 'input_continuous_all: (74)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 474)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'cin_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 474), output_shape (None, 128)\n",
      "cin: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-16 14:20:34 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "1118/1118 - 37s - loss: 4.1703 - root_mean_squared_error: 2.0421 - val_loss: 3.5986 - val_root_mean_squared_error: 1.8970 - lr: 0.0010 - 37s/epoch - 33ms/step\n",
      "11-16 14:21:12 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-16 14:21:12 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-16 14:21:13 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:21:13 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:21:13 I deeptables.m.preprocessor.py 251 - transform_X taken 0.20813632011413574s\n",
      "11-16 14:21:13 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:21:13 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "67/67 [==============================] - 2s 15ms/step\n",
      "11-16 14:21:14 I deeptables.m.deeptable.py 559 - predict_proba taken 1.925229787826538s\n",
      "\n",
      "Fold 3 | rmse: 1.8961\n",
      "\n",
      "#########################\n",
      "### Fold 4/5 ...\n",
      "#########################\n",
      "11-16 14:21:15 I deeptables.m.deeptable.py 338 - X.Shape=(143266, 74), y.Shape=(143266,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'cin_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7832c039a8f0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-16 14:21:15 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-16 14:21:15 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:21:16 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-16 14:21:16 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.12622928619384766s\n",
      "11-16 14:21:16 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-16 14:21:16 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.001085519790649414s\n",
      "11-16 14:21:16 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-16 14:21:16 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16584\n",
      "[LightGBM] [Info] Number of data points in the train set: 143266, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score 8.316733\n",
      "11-16 14:21:22 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 6.384299993515015s\n",
      "11-16 14:21:22 I deeptables.m.preprocessor.py 198 - fit_transform taken 6.868587493896484s\n",
      "11-16 14:21:22 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:21:23 I deeptables.m.preprocessor.py 251 - transform_X taken 0.21086764335632324s\n",
      "11-16 14:21:23 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-16 14:21:23 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0002675056457519531s\n",
      "11-16 14:21:23 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "11-16 14:21:23 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:21:24 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:21:24 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-16 14:21:25 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (100)', 'input_continuous_all: (74)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 474)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'cin_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 474), output_shape (None, 128)\n",
      "cin: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-16 14:21:25 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "1119/1119 - 38s - loss: 4.4587 - root_mean_squared_error: 2.1116 - val_loss: 4.9238 - val_root_mean_squared_error: 2.2190 - lr: 0.0010 - 38s/epoch - 34ms/step\n",
      "11-16 14:22:04 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-16 14:22:04 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-16 14:22:05 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:05 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:05 I deeptables.m.preprocessor.py 251 - transform_X taken 0.21238255500793457s\n",
      "11-16 14:22:05 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:05 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "66/66 [==============================] - 2s 19ms/step\n",
      "11-16 14:22:07 I deeptables.m.deeptable.py 559 - predict_proba taken 2.1965408325195312s\n",
      "\n",
      "Fold 4 | rmse: 2.2177\n",
      "\n",
      "#########################\n",
      "### Fold 5/5 ...\n",
      "#########################\n",
      "11-16 14:22:07 I deeptables.m.deeptable.py 338 - X.Shape=(127783, 74), y.Shape=(127783,), batch_size=128, config=ModelConfig(name='conf-1', nets=['dnn_nets', 'cin_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7832c039a8f0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-16 14:22:07 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-16 14:22:08 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:22:08 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-16 14:22:08 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.12134313583374023s\n",
      "11-16 14:22:08 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-16 14:22:08 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.0007140636444091797s\n",
      "11-16 14:22:08 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-16 14:22:08 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16689\n",
      "[LightGBM] [Info] Number of data points in the train set: 127783, number of used features: 74\n",
      "[LightGBM] [Info] Start training from score 8.164584\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 5.505497455596924s\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 198 - fit_transform taken 5.941567659378052s\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 251 - transform_X taken 0.3005838394165039s\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-16 14:22:14 I deeptables.m.preprocessor.py 238 - transform_y taken 0.0007836818695068359s\n",
      "11-16 14:22:15 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "11-16 14:22:15 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:22:16 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-16 14:22:16 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-16 14:22:17 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (100)', 'input_continuous_all: (74)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 474)\n",
      "---------------------------------------------------------\n",
      "nets: ['dnn_nets', 'cin_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "dnn: input_shape (None, 474), output_shape (None, 128)\n",
      "cin: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 100, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-16 14:22:17 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "998/998 - 36s - loss: 3.9488 - root_mean_squared_error: 1.9872 - val_loss: 4.6684 - val_root_mean_squared_error: 2.1606 - lr: 0.0010 - 36s/epoch - 36ms/step\n",
      "11-16 14:22:53 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-16 14:22:53 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-16 14:22:54 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:54 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:55 I deeptables.m.preprocessor.py 251 - transform_X taken 0.27289605140686035s\n",
      "11-16 14:22:55 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:55 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "97/97 [==============================] - 2s 15ms/step\n",
      "11-16 14:22:57 I deeptables.m.deeptable.py 559 - predict_proba taken 2.555514097213745s\n",
      "\n",
      "Fold 5 | rmse: 2.1609\n",
      "\n",
      "Overall CV rmse: 2.1864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_models = train_nn(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb188a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:22:58.053716Z",
     "iopub.status.busy": "2024-11-16T14:22:58.053369Z",
     "iopub.status.idle": "2024-11-16T14:22:59.717698Z",
     "shell.execute_reply": "2024-11-16T14:22:59.716820Z"
    },
    "papermill": {
     "duration": 1.698896,
     "end_time": "2024-11-16T14:22:59.719681",
     "exception": false,
     "start_time": "2024-11-16T14:22:58.020785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-16 14:22:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 251 - transform_X taken 0.03461956977844238s\n",
      "11-16 14:22:58 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:58 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "8/8 [==============================] - 0s 31ms/step\n",
      "11-16 14:22:58 I deeptables.m.deeptable.py 559 - predict_proba taken 0.466357946395874s\n",
      "11-16 14:22:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 251 - transform_X taken 0.035807132720947266s\n",
      "11-16 14:22:58 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:58 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "11-16 14:22:58 I deeptables.m.deeptable.py 559 - predict_proba taken 0.338484525680542s\n",
      "11-16 14:22:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:58 I deeptables.m.preprocessor.py 251 - transform_X taken 0.04004526138305664s\n",
      "11-16 14:22:58 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:58 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "11-16 14:22:59 I deeptables.m.deeptable.py 559 - predict_proba taken 0.3419320583343506s\n",
      "11-16 14:22:59 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:59 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:59 I deeptables.m.preprocessor.py 251 - transform_X taken 0.03787708282470703s\n",
      "11-16 14:22:59 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:59 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "11-16 14:22:59 I deeptables.m.deeptable.py 559 - predict_proba taken 0.3404355049133301s\n",
      "11-16 14:22:59 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-16 14:22:59 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-16 14:22:59 I deeptables.m.preprocessor.py 251 - transform_X taken 0.0363461971282959s\n",
      "11-16 14:22:59 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-16 14:22:59 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "11-16 14:22:59 I deeptables.m.deeptable.py 559 - predict_proba taken 0.16269731521606445s\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test[feature_cols]\n",
    "\n",
    "nn_predictions = infer_nn(X_test, nn_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67bf14a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T14:22:59.788756Z",
     "iopub.status.busy": "2024-11-16T14:22:59.788451Z",
     "iopub.status.idle": "2024-11-16T14:22:59.806342Z",
     "shell.execute_reply": "2024-11-16T14:22:59.805227Z"
    },
    "papermill": {
     "duration": 0.054066,
     "end_time": "2024-11-16T14:22:59.808445",
     "exception": false,
     "start_time": "2024-11-16T14:22:59.754379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id    bg+1:00\n",
      "0  p01_8459   8.866250\n",
      "1  p01_8460   5.327205\n",
      "2  p01_8461   7.848865\n",
      "3  p01_8462  10.579509\n",
      "4  p01_8463   6.666002\n"
     ]
    }
   ],
   "source": [
    "df_subm['bg+1:00'] = nn_predictions\n",
    "df_subm.to_csv('deeptables_submission.csv', index=False)\n",
    "print(df_subm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a99bfc",
   "metadata": {
    "papermill": {
     "duration": 0.032996,
     "end_time": "2024-11-16T14:22:59.877525",
     "exception": false,
     "start_time": "2024-11-16T14:22:59.844529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9553358,
     "sourceId": 82611,
     "sourceType": "competition"
    },
    {
     "datasetId": 4074593,
     "sourceId": 7074842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4021289,
     "sourceId": 7570020,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 203746103,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 414.844144,
   "end_time": "2024-11-16T14:23:03.613581",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-16T14:16:08.769437",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
